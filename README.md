# Drone_Control_Using_Poses with Python

**YouTube Link:** https://youtu.be/Uu_Br9ynfZw


This project constitutes the **Final Degree Project (TFG)** in Telecommunication Systems Engineering. The main objective was to develop a control system for a drone using **pose recognition** captured through a camera, whether through hand gestures, body positions, or limb movements.

The project was based on detecting key points of the human body. By using advanced algorithms and **Big Data** scripts, it was possible to accurately identify the hands and body of any person in various situations. This enabled the programming of a system that compares the position of the key points and determines the specific pose, triggering different drone movements based on the detected gestures.

Additionally, the project included interactive elements, such as an on-screen scoring system that updated in **real-time** based on interactions with the drone. To enrich the user experience, sound effects and music were also incorporated, adding a playful and immersive component to controlling the drone.

This project not only achieved the proposed technical objectives but also marked significant personal and academic learning in several key areas. Among the accomplished goals are:

- **Acquisition of Python skills:** As a first experience in programming with this language, the project provided a solid understanding of its syntax, structures, and tools, which was crucial for developing the necessary algorithms.

- **Understanding drone operation:** The project delved into the operational structure and control system of drones, gaining insights into how they interact with external signals and respond dynamically to commands based on sensor data and visual analysis.
  
- **Implementation of computer vision algorithms:** Advanced scripts for detecting key points of the human body were understood and applied, integrating real-time data analysis and visual processing techniques.
  
- **Design and integration of interactive systems:** An interactive interface was developed, combining physical interaction (gestures and poses) with visual and auditory elements, such as on-screen scoring and music, enhancing the user experience.
  
- **Development of competencies in Big Data and real-time processing:** The use of algorithms to analyze and process large volumes of visual data under various conditions allowed for a deeper understanding of these technologies and their practical applications.
  
- **Problem-solving in dynamic environments:** The project fostered problem-solving skills, addressing challenges such as accurate detection under different lighting conditions, body positions, and user variability.

This work was an enriching experience, not only from a technical perspective but also in terms of interdisciplinary learning, combining knowledge of telecommunications, programming, and artificial vision to develop a functional, interactive, and innovative system.
